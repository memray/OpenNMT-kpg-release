{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About\n",
    "The goal of this script is to process a few common keyphrase datasets, including\n",
    " - **Tokenize**: by default using method from Meng et al. 2017, which fits more for academic text since it splits strings by hyphen etc. and makes tokens more fine-grained. \n",
    "     - keep [_<>,\\(\\)\\.\\'%]\n",
    "     - replace digits with < digit >\n",
    "     - split by [^a-zA-Z0-9_<>,#&\\+\\*\\(\\)\\.\\'%]\n",
    " - **Determine present/absent phrases**: determine whether a phrase appears verbatim in a text. This is believed a very important step for the evaluation of keyphrase-related tasks, since in general extraction methods cannot recall any phrases don't appear in the source text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "module_path = os.path.abspath(os.path.join('../onmt'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import kp_evaluate\n",
    "import onmt.keyphrase.utils as utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inspec\n",
      "#doc=500, #present_doc=497, #absent_doc=381, #tgt=4913, #present=3858, #absent=1055\n",
      "krapivin\n",
      "#doc=460, #present_doc=437, #absent_doc=417, #tgt=2641, #present=1485, #absent=1156\n",
      "nus\n",
      "#doc=211, #present_doc=207, #absent_doc=195, #tgt=2461, #present=1263, #absent=1198\n",
      "semeval\n",
      "#doc=100, #present_doc=100, #absent_doc=99, #tgt=1507, #present=671, #absent=836\n",
      "kp20k\n",
      "#doc=19987, #present_doc=19048, #absent_doc=16357, #tgt=105181, #present=66595, #absent=38586\n",
      "duc\n",
      "#doc=308, #present_doc=308, #absent_doc=38, #tgt=2484, #present=2421, #absent=63\n",
      "stackexchange\n",
      "#doc=16000, #present_doc=13475, #absent_doc=10984, #tgt=43131, #present=24809, #absent=18322\n"
     ]
    }
   ],
   "source": [
    "dataset_names = ['inspec', 'krapivin', 'nus', 'semeval', 'kp20k', 'duc', 'stackexchange']\n",
    "\n",
    "json_base_dir = '/Users/memray/project/kp/OpenNMT-kpg/data/keyphrase/json/' # path to the json folder\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    print(dataset_name)\n",
    "    \n",
    "    input_json_path = os.path.join(json_base_dir, dataset_name, '%s_test.json' % dataset_name)\n",
    "    output_json_path = os.path.join(json_base_dir, dataset_name, '%s_test_meng17token.json' % dataset_name)\n",
    "\n",
    "    doc_count, present_doc_count, absent_doc_count = 0, 0, 0\n",
    "    tgt_num, present_tgt_num, absent_tgt_num = [], [], []\n",
    "    \n",
    "    with open(input_json_path, 'r') as input_json, open(output_json_path, 'w') as output_json:\n",
    "        for json_line in input_json:\n",
    "            json_dict = json.loads(json_line)\n",
    "\n",
    "            if dataset_name == 'stackexchange':\n",
    "                json_dict['abstract'] = json_dict['question']\n",
    "                json_dict['keywords'] = json_dict['tags']            \n",
    "                del json_dict['question']\n",
    "                del json_dict['tags']\n",
    "\n",
    "            title = json_dict['title']\n",
    "            abstract = json_dict['abstract']\n",
    "            keywords = json_dict['keywords']\n",
    "\n",
    "            if isinstance(keywords, str):\n",
    "                keywords = keywords.split(';')\n",
    "                json_dict['keywords'] = keywords\n",
    "            # remove all the abbreviations/acronyms in parentheses in keyphrases\n",
    "            keywords = [re.sub(r'\\(.*?\\)|\\[.*?\\]|\\{.*?\\}', '', kw) for kw in keywords]\n",
    "            \n",
    "            # tokenize text\n",
    "            title_token = utils.meng17_tokenize(title)\n",
    "            abstract_token = utils.meng17_tokenize(abstract)\n",
    "            keywords_token = [utils.meng17_tokenize(kw) for kw in keywords]\n",
    "\n",
    "            # replace numbers\n",
    "            title_token = utils.replace_numbers_to_DIGIT(title_token, k=2)\n",
    "            abstract_token = utils.replace_numbers_to_DIGIT(abstract_token, k=2)\n",
    "            keywords_token = [utils.replace_numbers_to_DIGIT(kw, k=2) for kw in keywords_token]                \n",
    "            \n",
    "            src_token = title_token+[\".\"]+abstract_token\n",
    "            tgts_token = keywords_token\n",
    "\n",
    "#             print(json_dict)\n",
    "#             print(src_token)\n",
    "#             print(tgts_token)\n",
    "\n",
    "            # split tgts by present/absent\n",
    "            src_seq = src_token\n",
    "            tgt_seqs = tgts_token\n",
    "            \n",
    "            present_tgt_flags, _, _ = if_present_duplicate_phrases(src_seq, tgt_seqs)\n",
    "            present_tgts = [tgt for tgt, present in zip(tgt_seqs, present_tgt_flags) if present]\n",
    "            absent_tgts = [tgt for tgt, present in zip(tgt_seqs, present_tgt_flags) if ~present]\n",
    "            \n",
    "            doc_count += 1\n",
    "            present_doc_count = present_doc_count + 1 if len(present_tgts) > 0 else present_doc_count\n",
    "            absent_doc_count = absent_doc_count + 1 if len(absent_tgts) > 0 else absent_doc_count\n",
    "            \n",
    "            tgt_num.append(len(tgt_seqs))\n",
    "            present_tgt_num.append(len(present_tgts))\n",
    "            absent_tgt_num.append(len(absent_tgts))\n",
    "            \n",
    "            # write to output json\n",
    "            tokenized_dict = {'src': src_token, 'tgt': tgts_token, \n",
    "                              'present_tgt': present_tgts, 'absent_tgt': absent_tgts}\n",
    "            json_dict['meng17_tokenized'] = tokenized_dict\n",
    "            output_json.write(json.dumps(json_dict) + '\\n')\n",
    "\n",
    "    print('#doc=%d, #present_doc=%d, #absent_doc=%d, #tgt=%d, #present=%d, #absent=%d' \n",
    "          % (doc_count, present_doc_count, absent_doc_count, \n",
    "             sum(tgt_num), sum(present_tgt_num), sum(absent_tgt_num)))\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
